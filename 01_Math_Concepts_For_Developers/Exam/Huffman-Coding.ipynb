{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huffman Coding\n",
    "## Author: Tsvetan Dimitrov\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Lossy vs Lossless Compression\n",
    "In this section we will look at lossy vs lossless compression and the advantages and disadvantages of both methods. There is no right or wrong method. It all comes down to taking some notice of a number of different factors. \n",
    "\n",
    "#### Lossy Compression\n",
    "The first type is lossy compression which refers to some of the data from the original file being lost after compression is executed. The process is irreversible and once you convert to lossy, you cannot go back. And the more you compress it, the more degradation occurs. JPEG and GIF are both lossy image formats. One of the biggest obvious benefits to using lossy compression is that it results in a significantly reduced file size (smaller than lossless compression method), but it also means there is quality loss. Most tools, plugins, and software out there will let you choose the degree of compression you want to use.\n",
    "\n",
    "#### Lossless Compression\n",
    "The other type is lossless compression which refers to compression without any data or quality loss. All of original data can be recovered when the file is uncompressed. RAW, BMP, GIF, and PNG are all lossless image formats. The big benefit to lossless compression is that you can retain the original quality of your data or images and still achieve a smaller file size. This is generally the technique of choice for text or spreadsheet files, where losing words or financial data could pose a problem. In this article we will explore and implement a lossless technique called Huffman Coding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Entropy\n",
    "In information theory, the major goal is for one person (a transmitter) to convey some message (over a channel) to another person (the receiver). To do so, the transmitter sends a series (possibly just one) partial messages that give clues towards the original message. The information content of one of these partial messages is a measure of how much uncertainty this resolves for the receiver. Let us try a simple experiment. We will assume that the weather is equally probable to be at any one of 4 possible states at any given moment. This translates to having the same probability of a state occurring which is 1/4 in our case.\n",
    "\n",
    "|             | Sunny | Cloudy | Rainy | Foggy |\n",
    "|-------------|-------|--------|-------|-------|\n",
    "| probability |  1/4  |  1/4   |  1/4  |  1/4  |\n",
    "\n",
    "Now the question is what is the minimal number of bits we can use to store each of these states based on their probability? Our probability can be expressed as follows: \n",
    "$$\\frac{1}{4} = \\frac{1}{2^2} = 2^{-2}$$\n",
    "\n",
    "$$\\text{minimum number of bits} = -\\log (p) = -\\log \\frac{1}{4} = 2$$\n",
    "So we can now add codes to our table for each weather state:\n",
    "\n",
    "|             | Sunny | Cloudy | Rainy | Foggy |\n",
    "|-------------|-------|--------|-------|-------|\n",
    "| probability |  1/4  |  1/4   |  1/4  |  1/4  |\n",
    "| code        |  00   |  01    |   10  |   11  |\n",
    "\n",
    "Each code has to uniquely identify the data that it is encoding, otherwise we will not be able to decode it correctly. Let us now try to change the probabilities and calculate their corresponding codes:\n",
    "\n",
    "|             | Sunny | Cloudy | Rainy | Foggy |\n",
    "|-------------|-------|--------|-------|-------|\n",
    "| probability |  1/2  |  1/4   |  1/8  |  1/8  |\n",
    "| code        |  0    |  10    |  110  |  111  |\n",
    "\n",
    "\n",
    "The fact that we can derive from our experiment is that the lower the probability value of a data source, the more information a message transfer event has to carry than a data source with a higher probability value. A partial message that cuts the number of possibilities in half transmits one bit of information about the message. In essence, the \"information content\" can be viewed as how much useful information the message actually contains. The entropy, in this context, is the expected number of bits of information contained in each message, taken over all possibilities for the transmitted message. In information theory entropy is denoted with $H$ and has the following definition:\n",
    "$$ H(X) = -p\\log (p)$$\n",
    "This gives us the weighted average of bits for the current partial message or state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "Huffman coding is a way to encode information using variable length strings to represent symbols depending on how frequently they appear. The idea is that symbols that are used more frequently should be shorter while symbols that appear more rarely can be longer. This way, the number of bits it takes to encode a given message will be shorter, on average, than if a fixed-length code was used. In messages that include many rare symbols, the string produced by variable-length encoding may be longer than one produced by a fixed-length encoding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeapNode:\n",
    "    def __init__(self, char, freq):\n",
    "        self.char = char\n",
    "        self.freq = freq\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "    \n",
    "    def __cmp__(self, other):\n",
    "        if other == None or not isinstance(other, HeapNode):\n",
    "            return -1\n",
    "        return self.freq > other.freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuffmanTree:\n",
    "    def __init__(self):\n",
    "        self.heap = []\n",
    "        self.frequency_dict = {}\n",
    "        \n",
    "    def __make_frequency_dict(text):\n",
    "        for char in text:\n",
    "            if not char in self.frequency_dict:\n",
    "                self.frequency_dict[char] = 0\n",
    "            self.frequency_dict[char] += 1\n",
    "    \n",
    "    def __make_heap():\n",
    "        for key in self.frequency_dict:\n",
    "            node = HeapNode(key, self.frequency_dict[key])\n",
    "            heapq.heappush(self.heap, node)\n",
    "            \n",
    "    def __merge_nodes():\n",
    "        while len(self.heap) > 1:\n",
    "            left_node = heapq.heappop(self.heap)\n",
    "            right_node = heapq.heappop(self.heap)\n",
    "\n",
    "            merged_node = HeapNode(None, left_node.freq + right_node.freq)\n",
    "            merged_node.left = left_node\n",
    "            merged_node.right = right_node\n",
    "\n",
    "            heapq.heappush(self.heap, merged_node)\n",
    "    \n",
    "    def construct_tree(text):\n",
    "        __make_frequency_dict(text)\n",
    "        __make_heap()\n",
    "        __merge_nodes()\n",
    "        \n",
    "        return self.heap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuffmanCodeMap():\n",
    "    def __init__(self, tree):\n",
    "        self.tree = tree\n",
    "        self.code_map = {}\n",
    "        self.reverse_code_map = {}\n",
    "    \n",
    "    def __make_codes(self, root, current_code):\n",
    "        if root == None:\n",
    "            return\n",
    "        \n",
    "        if root.char != None:\n",
    "            self.code_map[root.char] = current_code\n",
    "            self.reverse_code_map[current_code] = root.char\n",
    "            return\n",
    "        self.__make_codes(root.left, f'{current_code}0')\n",
    "        self.__make_codes(root.right, f'{current_code}1')\n",
    "        \n",
    "    def construct_code_maps(self):\n",
    "        root = heapq.headpop(self.tree)\n",
    "        current_code = ''\n",
    "        self.__make_codes(root, current_code)\n",
    "        return {\n",
    "            'code_map': code_map,\n",
    "            'reverse_code_map': reverse_code_map\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
