{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "import os\n",
    "from functools import total_ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huffman Coding\n",
    "## Author: Tsvetan Dimitrov\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Lossy vs Lossless Compression\n",
    "In this section we will look at lossy vs lossless compression and the advantages and disadvantages of both methods. There is no right or wrong method. It all comes down to taking some notice of a number of different factors. \n",
    "\n",
    "#### Lossy Compression\n",
    "The first type is lossy compression which refers to some of the data from the original file being lost after compression is executed. The process is irreversible and once you convert to lossy, you cannot go back. And the more you compress it, the more degradation occurs. JPEG and GIF are both lossy image formats. One of the biggest obvious benefits to using lossy compression is that it results in a significantly reduced file size (smaller than lossless compression method), but it also means there is quality loss. Most tools, plugins, and software out there will let you choose the degree of compression you want to use.\n",
    "\n",
    "#### Lossless Compression\n",
    "The other type is lossless compression which refers to compression without any data or quality loss. All of original data can be recovered when the file is uncompressed. RAW, BMP, GIF, and PNG are all lossless image formats. The big benefit to lossless compression is that you can retain the original quality of your data or images and still achieve a smaller file size. This is generally the technique of choice for text or spreadsheet files, where losing words or financial data could pose a problem. In this article we will explore and implement a lossless technique called Huffman Coding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Entropy\n",
    "In information theory, the major goal is for one person (a transmitter) to convey some message (over a channel) to another person (the receiver). To do so, the transmitter sends a series (possibly just one) partial messages that give clues towards the original message. The information content of one of these partial messages is a measure of how much uncertainty this resolves for the receiver. Let us try a simple experiment. We will assume that the weather is equally probable to be at any one of 4 possible states at any given moment. This translates to having the same probability of a state occurring which is 1/4 in our case.\n",
    "\n",
    "|             | Sunny | Cloudy | Rainy | Foggy |\n",
    "|-------------|-------|--------|-------|-------|\n",
    "| probability |  1/4  |  1/4   |  1/4  |  1/4  |\n",
    "\n",
    "Now the question is what is the minimal number of bits we can use to store each of these states based on their probability? Our probability can be expressed as follows: \n",
    "$$\\frac{1}{4} = \\frac{1}{2^2} = 2^{-2}$$\n",
    "\n",
    "$$\\text{minimum number of bits} = -\\log (p) = -\\log \\frac{1}{4} = 2$$\n",
    "So we can now add codes to our table for each weather state:\n",
    "\n",
    "|             | Sunny | Cloudy | Rainy | Foggy |\n",
    "|-------------|-------|--------|-------|-------|\n",
    "| probability |  1/4  |  1/4   |  1/4  |  1/4  |\n",
    "| code        |  00   |  01    |   10  |   11  |\n",
    "\n",
    "Each code has to uniquely identify the data that it is encoding, otherwise we will not be able to decode it correctly. Let us now try to change the probabilities and calculate their corresponding codes:\n",
    "\n",
    "|             | Sunny | Cloudy | Rainy | Foggy |\n",
    "|-------------|-------|--------|-------|-------|\n",
    "| probability |  1/2  |  1/4   |  1/8  |  1/8  |\n",
    "| code        |  0    |  10    |  110  |  111  |\n",
    "\n",
    "\n",
    "The fact that we can derive from our experiment is that the lower the probability value of a data source, the more information a message transfer event has to carry than a data source with a higher probability value. A partial message that cuts the number of possibilities in half transmits one bit of information about the message. In essence, the \"information content\" can be viewed as how much useful information the message actually contains. The entropy, in this context, is the expected number of bits of information contained in each message, taken over all possibilities for the transmitted message. In information theory entropy is denoted with $H$ and has the following definition:\n",
    "$$ H(X) = -p\\log (p)$$\n",
    "This gives us the weighted average of bits for the current partial message or state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm overview\n",
    "Huffman coding is a way to encode information using variable length strings to represent symbols depending on how frequently they appear. The idea is that symbols that are used more frequently should be shorter while symbols that appear more rarely can be longer. This way, the number of bits it takes to encode a given message will be shorter, on average, than if a fixed-length code was used. In messages that include many rare symbols, the string produced by variable-length encoding may be longer than one produced by a fixed-length encoding. The steps that the algorithm executes are the following:\n",
    "\n",
    "**Step 1**. Take a list of symbols and their probabilities.\n",
    "\n",
    "**Step 2**. Select two symbols with the lowest probabilities (if multiple symbols have the same probability, select two arbitrarily).\n",
    "\n",
    "**Step 3**. Create a binary tree out of these two symbols, labeling one branch with a \"1\" and the other with a \"0\". It doesn't matter which side you label 1 or 0 as long as the labeling is consistent throughout the problem (e.g. the left side should always be 1 and the right side should always be 0, or the left side should always be 0 and the right side should always be 1).\n",
    "\n",
    "**Step 4**. Add the probabilities of the two symbols to get the probability of the new subtree.\n",
    "\n",
    "**Step 5**. Remove the symbols from the list and add the subtree to the list.\n",
    "\n",
    "**Step 6**. Go back through the list and take the two symbols/subtrees with the smallest probabilities and combine those into a new subtree. Remove the original symbols/subtrees from the list, and add the new subtree to the list.\n",
    "\n",
    "**Step 7**. Repeat until all of the elements are combined.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Huffman coding uses a greedy algorithm to build a prefix tree that optimizes the encoding scheme so that the most frequently used symbols have the shortest encoding. The prefix tree describing the encoding ensures that the code for any particular symbol is never a prefix of the bit string representing any other symbol. First step is to create a tree node that will carry the needed information and be able to connect to other nodes. We will need the symbol itself, its frequency count and pointers to its direct left and right child:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@total_ordering\n",
    "class Node:\n",
    "    def __init__(self, char, freq):\n",
    "        self.char = char\n",
    "        self.freq = freq\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        return self.freq < other.freq\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if other == None or not isinstance(other, Node):\n",
    "            return False\n",
    "        \n",
    "        return self.freq == other.freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "NOTE: Since we have a custom object as our data structure we need to define how two objects should compare by defining `__eq__` for equality and `__lt__` for less than. `@total_ordering` takes care of the rest of the comparison operators by infering the opposites of the already defined ones.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree Construction\n",
    "\n",
    "Now we will need a tree structure, connecting our nodes. In order to determine the binary assignment for a symbol we have to make the leaves of the tree correspond to the symbols and the assignment will be the path it takes to get from the root of the tree to that leaf. The tree construction steps are the following:\n",
    "\n",
    "**Step 1**. Count the frequency counts of all symbols and store them in a dictionary.\n",
    "\n",
    "**Step 2**. Construct a new `Node` for each symbol in the dictionary.\n",
    "\n",
    "**Step 3**. Push each new `Node` into a Min Heap data structure.\n",
    "\n",
    "**Step 4**. Connect tree nodes by popping the two least frequent symbol nodes from the heap and sum their probabilities into a new parent `Node`.\n",
    "\n",
    "**Step 5**. Push the newly constructed parent `Node` in the heap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuffmanTree:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.heap = []\n",
    "        self.frequency_dict = {}\n",
    "        \n",
    "    def __make_frequency_dict(self):\n",
    "        for char in self.text:\n",
    "            if not char in self.frequency_dict:\n",
    "                self.frequency_dict[char] = 0\n",
    "            self.frequency_dict[char] += 1\n",
    "                \n",
    "    def __make_heap(self):\n",
    "        for key in self.frequency_dict:\n",
    "            node = Node(key, self.frequency_dict[key])\n",
    "            heapq.heappush(self.heap, node)\n",
    "                    \n",
    "    def __merge_nodes(self):\n",
    "        while len(self.heap) > 1:\n",
    "            left_node = heapq.heappop(self.heap)\n",
    "            right_node = heapq.heappop(self.heap)\n",
    "\n",
    "            merged_node = Node(None, left_node.freq + right_node.freq)\n",
    "            merged_node.left = left_node\n",
    "            merged_node.right = right_node\n",
    "\n",
    "            heapq.heappush(self.heap, merged_node)\n",
    "    \n",
    "    def construct_tree(self):\n",
    "        self.__make_frequency_dict()\n",
    "        self.__make_heap()\n",
    "        self.__merge_nodes()\n",
    "        return self.heap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symbol Code Generation\n",
    "\n",
    "The next step is to generate all the codes for our symbols. We have already constructed our binary tree with summed probabilities. Now we have to traverse it from top to bottom. In our case we choose that all left nodes will add a `0` to their code at each depth level and all right nodes will add a `1`. Once we reach a leaf we store an entry of the form `<symbol>: <code>` in a dictionary and its reverse `<code>: <symbol>` in another dictionary which will be useful for decompression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuffmanCodeMap:\n",
    "    def __init__(self, tree):\n",
    "        self.tree = tree\n",
    "        self.code_map = {}\n",
    "        self.reverse_code_map = {}\n",
    "    \n",
    "    def __make_codes(self, root, current_code):\n",
    "        if root == None:\n",
    "            return\n",
    "        \n",
    "        if root.char != None:\n",
    "            self.code_map[root.char] = current_code\n",
    "            self.reverse_code_map[current_code] = root.char\n",
    "            return\n",
    "        self.__make_codes(root.left, f'{current_code}0')\n",
    "        self.__make_codes(root.right, f'{current_code}1')\n",
    "        \n",
    "    def construct_code_maps(self):\n",
    "        root = heapq.heappop(self.tree)\n",
    "        current_code = ''\n",
    "        \n",
    "        self.__make_codes(root, current_code)\n",
    "        return {\n",
    "            'code_map': self.code_map,\n",
    "            'reverse_code_map': self.reverse_code_map\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bit Padding\n",
    "\n",
    "We are almost ready to implement our `compress` and `decompress` functions. But we have to bare in mind something about memory which could lead to data corruption if not considered. For example, suppose we want to encode the word **bird**. This could result in the following sequence of thirteen bits:\n",
    "```\n",
    "1101001100111\n",
    "```\n",
    "When stored on disk the sequence would be padded with 3 additional random bits in order to achieve a count that is a multiple of 8 (which is a power of 2). If those bits were, e.g. `111` we would get:\n",
    "```\n",
    "1101001100111111\n",
    "```\n",
    "If we were to load this back from disk we will not get the original string but something like **birdk**. To fix this problem, we have to have some way of knowing when we have finished reading back all of the bits that encode our sequence. One way of doing this is to reserve 8 bits at the beginning that will store the number of bits used as padding at the end. Of course we have to remove this padding when decompressing because it is not needed. We do that by reading those first 8 bits, convert them back into a number that shows the count of padding bits and then simply cut that amount of bits off from the end of the bit sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(encoded_text):\n",
    "    extra_padding = 8 - len(encoded_text) % 8\n",
    "    encoded_text += (extra_padding * '0')\n",
    "\n",
    "    padded_info = f'{extra_padding:08b}'\n",
    "    return padded_info + encoded_text\n",
    "\n",
    "def remove_padding(padded_encoded_text):\n",
    "    padded_info = padded_encoded_text[:8]\n",
    "    extra_padding = int(padded_info, 2)\n",
    "\n",
    "    padded_encoded_text = padded_encoded_text[8:]\n",
    "    return padded_encoded_text[:-extra_padding]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compression\n",
    "\n",
    "#### Decompression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuffmanCoding:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.code_map = {}\n",
    "        self.reverse_code_map = {}\n",
    "    \n",
    "    def __to_bytes(self, padded_encoded_text):\n",
    "        text_len = len(padded_encoded_text)\n",
    "        \n",
    "        if text_len % 8 != 0:\n",
    "            print(\"Encoded text not padded properly\")\n",
    "            exit(0)\n",
    "            \n",
    "        b = bytearray()\n",
    "        for i in range(0, text_len, 8):\n",
    "            byte = padded_encoded_text[i:i + 8]\n",
    "            b.append(int(byte, 2))\n",
    "        return b\n",
    "    \n",
    "    def __encode_text(self, text):\n",
    "        encoded_text = ''\n",
    "        for char in text:\n",
    "            encoded_text += self.code_map[char]\n",
    "        return add_padding(encoded_text)\n",
    "    \n",
    "    def __decode_text(self, encoded_text):\n",
    "        current_code = ''\n",
    "        decoded_text = ''\n",
    "        \n",
    "        for bit in encoded_text:\n",
    "            current_code += bit\n",
    "            if current_code in self.reverse_code_map:\n",
    "                char = self.reverse_code_map[current_code]\n",
    "                decoded_text += char\n",
    "                current_code = ''\n",
    "        \n",
    "        return decoded_text\n",
    "    \n",
    "    def compress(self):\n",
    "        file_name, _ = os.path.splitext(self.path)\n",
    "        output_path = f'{file_name}.bin'\n",
    "        \n",
    "        with open(self.path, 'r+') as input_file, open(output_path, 'wb') as output_file:\n",
    "            text = input_file.read().rstrip()\n",
    "            tree = HuffmanTree(text).construct_tree()\n",
    "            codes = HuffmanCodeMap(tree).construct_code_maps()\n",
    "            self.code_map = codes['code_map']\n",
    "            self.reverse_code_map = codes['reverse_code_map']\n",
    "            \n",
    "            encoded_text = self.__encode_text(text)\n",
    "            \n",
    "            b = self.__to_bytes(encoded_text)\n",
    "            \n",
    "            output_file.write(bytes(b))\n",
    "        \n",
    "        print(f\"File '{self.path}' is compressed as '{output_path}'\")\n",
    "        \n",
    "        return output_path\n",
    "    \n",
    "    def decompress(self, input_path):\n",
    "        file_name, _ = os.path.splitext(self.path)\n",
    "        output_path = f'{file_name}_decompressed.txt'\n",
    "        \n",
    "        with open(input_path, 'rb') as input_file, open(output_path, 'w') as output_file:\n",
    "            bit_string = ''\n",
    "            \n",
    "            byte = input_file.read(1)\n",
    "            while byte:\n",
    "                byte = ord(byte)\n",
    "                bits = bin(byte)[2:].rjust(8, '0')\n",
    "                bit_string += bits\n",
    "                byte = input_file.read(1)\n",
    "                \n",
    "            encoded_text = remove_padding(bit_string)\n",
    "            decompressed_text = self.__decode_text(encoded_text)\n",
    "            \n",
    "            output_file.write(decompressed_text)\n",
    "            \n",
    "        print(f\"File '{input_path}' is decompressed as '{output_path}'\")\n",
    "        \n",
    "        return output_path\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'sample.txt' is compressed as 'sample.bin'\n",
      "File 'sample.bin' is decompressed as 'sample_decompressed.txt'\n",
      "394.017KB\n",
      "715.332KB\n"
     ]
    }
   ],
   "source": [
    "path = 'sample.txt'\n",
    "h = HuffmanCoding(path)\n",
    "output_path = h.compress()\n",
    "h.decompress(output_path)\n",
    "\n",
    "def get_file_size(path):\n",
    "    file_size = os.path.getsize(path)\n",
    "    return f'{file_size / 1000}KB'\n",
    "\n",
    "print(get_file_size('sample.bin'))\n",
    "print(get_file_size('sample_decompressed.txt'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
